"""
Etos Scraper Agent
LangGraph-powered intelligent scraping agent for Etos drugstore
"""

from typing import Dict, List, Any, Optional
from datetime import datetime
import asyncio
import random
import aiohttp
import json
import xml.etree.ElementTree as ET
import re
import logging
from bs4 import BeautifulSoup
from pathlib import Path

from langgraph.graph import StateGraph
from langgraph.graph import END
from agents.base_agent import BaseAgent, AgentState, AgentStatus

logger = logging.getLogger(__name__)

class EtosScrapingState(AgentState):
    """Etos specific scraping state"""
    store_name: str = "Etos"
    store_slug: str = "etos"
    base_url: str = "https://www.etos.nl"
    sitemap_url: str = "https://www.etos.nl/sitemap_0-product.xml"
    product_urls: List[str] = None
    products: List[Dict] = None
    current_url_batch: List[str] = None
    current_batch_num: int = 0
    batches_processed: int = 0
    retry_count: int = 0
    max_retries: int = 3
    rate_limit_delay: float = 1.0
    last_request_time: datetime = None
    session: aiohttp.ClientSession = None

class EtosAgent(BaseAgent):
    """Intelligent scraper agent for Etos drugstore"""
    
    def __init__(self):
        super().__init__("etos", "Etos Scraper Agent")
        self.state = EtosScrapingState()
        self.graph = self._build_graph()
        
        # Etos scraping configuration
        self.max_concurrent = 5
        self.batch_size = 10  # Process 10 products at a time
        self.delay_range = (1, 3)
        
        # Headers for Etos requests
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-User': '?1',
            'Sec-Fetch-Dest': 'document',
            'Cache-Control': 'max-age=0'
        }
        
        # Progress tracking
        self.successful_requests = 0
        self.failed_requests = 0
        self.rate_limited = 0
        
    def _build_graph(self) -> StateGraph:
        """Build the LangGraph state machine for Etos scraping"""
        
        # Define the workflow graph
        workflow = StateGraph(EtosScrapingState)
        
        # Add nodes (states)
        workflow.add_node("initialize", self._initialize_scraping)
        workflow.add_node("fetch_product_urls", self._fetch_product_urls)
        workflow.add_node("process_batch", self._process_batch)
        workflow.add_node("handle_rate_limit", self._handle_rate_limit)
        workflow.add_node("handle_error", self._handle_error)
        workflow.add_node("complete", self._complete_scraping)
        
        # Define the flow
        workflow.set_entry_point("initialize")
        
        workflow.add_edge("initialize", "fetch_product_urls")
        
        # Product URL processing with conditional logic
        workflow.add_conditional_edges(
            "fetch_product_urls",
            self._should_process_products,
            {
                "process": "process_batch",
                "complete": "complete"
            }
        )
        
        # Batch processing with error handling
        workflow.add_conditional_edges(
            "process_batch",
            self._check_batch_result,
            {
                "rate_limit": "handle_rate_limit",
                "error": "handle_error",
                "continue": "process_batch",
                "complete": "complete"
            }
        )
        
        workflow.add_edge("handle_rate_limit", "process_batch")
        workflow.add_edge("handle_error", "process_batch")
        workflow.add_edge("complete", END)
        
        return workflow.compile()
    
    async def create_session(self):
        """Create aiohttp session with proper configuration"""
        connector = aiohttp.TCPConnector(
            limit=self.max_concurrent,
            limit_per_host=self.max_concurrent,
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        
        self.state.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=self.headers
        )
        logger.info("Created Etos scraping session")

    async def close_session(self):
        """Close aiohttp session"""
        if self.state.session:
            await self.state.session.close()
            logger.info("Closed Etos scraping session")

    async def extract_product_urls_from_xml(self, xml_url: str) -> List[str]:
        """Extract product URLs from XML sitemap"""
        try:
            logger.info(f"Fetching XML sitemap from: {xml_url}")
            
            async with self.state.session.get(xml_url) as response:
                if response.status == 200:
                    content = await response.text()
                    root = ET.fromstring(content)
                    
                    # Handle sitemap namespaces
                    namespaces = {
                        'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9',
                        '': 'http://www.sitemaps.org/schemas/sitemap/0.9'
                    }
                    
                    product_urls = []
                    for url_elem in root.findall('.//ns:url/ns:loc', namespaces):
                        if url_elem is not None and '/producten/' in url_elem.text:
                            product_urls.append(url_elem.text)
                    
                    logger.info(f"Found {len(product_urls)} product URLs in XML sitemap")
                    return product_urls
                else:
                    logger.error(f"Failed to download sitemap. Status: {response.status}")
                    return []
        except Exception as e:
            logger.error(f"Error parsing XML sitemap: {str(e)}")
            return []

    def extract_product_id_from_url(self, url: str) -> str:
        """Extract product ID from URL"""
        try:
            # Example URL: https://www.etos.nl/producten/clearblue-snelle-detectie-zwangerschapstest-2-stuks-110001864.html
            if '/producten/' in url:
                # Try extracting the numeric ID at the end (most common pattern)
                id_match = re.search(r'-(\d+)\.html', url)
                if id_match:
                    return id_match.group(1)
                
                # If that fails, try extracting any numeric sequence at the end of the URL path
                path = url.split('?')[0]  # Remove any query parameters
                path = path.rstrip('/')   # Remove trailing slash if any
                segments = path.split('/')
                if segments:
                    last_segment = segments[-1].split('.')[0]  # Remove file extension
                    nums = re.findall(r'\d+', last_segment)
                    if nums:
                        # Return the last numeric sequence which is typically the product ID
                        return nums[-1]
            
            # For other URL formats, try to find any numeric sequence that might be a product ID
            nums = re.findall(r'\d{5,}', url)  # Look for numeric sequences of 5+ digits
            if nums:
                return nums[0]
                
            logger.warning(f"Could not extract product ID from URL: {url}")
            return ""
        except Exception as e:
            logger.error(f"Error extracting product ID from URL {url}: {str(e)}")
            return ""

    async def extract_product_data(self, url: str) -> Optional[Dict[str, Any]]:
        """Extract product data from a single product page using BeautifulSoup"""
        try:
            product_id = self.extract_product_id_from_url(url)
            logger.info(f"Extracting product data from: {url} (ID: {product_id})")
            
            # Add delay to avoid rate limiting
            await asyncio.sleep(random.uniform(*self.delay_range))
            
            async with self.state.session.get(url) as response:
                if response.status == 200:
                    html_content = await response.text()
                    soup = BeautifulSoup(html_content, 'html.parser')
                    
                    # Extract product data using CSS selectors
                    product_data = await self._extract_with_css(soup, url, product_id)
                    
                    if product_data:
                        self.successful_requests += 1
                        logger.info(f"‚úÖ Successfully extracted: {product_data.get('name', 'Unknown')}")
                        return product_data
                    else:
                        logger.warning(f"‚ùå No product data extracted from {url}")
                        self.failed_requests += 1
                        return None
                        
                elif response.status == 429:
                    logger.warning(f"‚è≥ Rate limited for {url}")
                    self.rate_limited += 1
                    return None
                else:
                    logger.error(f"‚ùå HTTP {response.status} for {url}")
                    self.failed_requests += 1
                    return None
                    
        except Exception as e:
            logger.error(f"‚ùå Error extracting product from {url}: {str(e)}")
            self.failed_requests += 1
            return None

    async def _extract_with_css(self, soup: BeautifulSoup, url: str, product_id: str) -> Optional[Dict[str, Any]]:
        """Extract product data using CSS selectors (based on original etos_scraper.py)"""
        try:
            # Product name
            name_element = soup.select_one("#product-title")
            name = name_element.get_text(strip=True) if name_element else None
            
            # Price
            price_element = soup.select_one(".price__value")
            price_text = price_element.get_text(strip=True) if price_element else None
            price = self._parse_price(price_text) if price_text else None
            
            # Original price (in case of discount)
            original_price_element = soup.select_one(".price__item--reference .price__value")
            original_price_text = original_price_element.get_text(strip=True) if original_price_element else None
            original_price = self._parse_price(original_price_text) if original_price_text else None
            
            # Content amount/Size
            content_element = soup.select_one(".product-tile__extra-label") or soup.select_one("span:contains('ML')")
            content = None
            if content_element:
                content_text = content_element.get_text(strip=True)
                content = content_text if content_text else None
            
            # If content not found in dedicated element, try to extract from title
            if not content and name:
                content_match = re.search(r'(\d+\s*(?:ML|ml|G|g|stuks|STUKS|st))', name)
                if content_match:
                    content = content_match.group(1)
            
            # Description
            description_element = soup.select_one(".accordion__item-content.s-rich-text") or soup.select_one(".s-rich-text")
            description = None
            if description_element:
                description_text = description_element.get_text(strip=True)
                description = description_text if description_text else None
            
            # Brand
            brand_element = soup.select_one(".product-details__brand-link")
            brand = None
            if brand_element:
                brand_text = brand_element.get_text(strip=True)
                brand = brand_text if brand_text else None
            
            # If brand is not found, try to extract from name
            if not brand and name:
                brand_candidates = ["Etos", "Clearblue", "Durex", "Maybelline", "L'Oreal", "Nivea", "Dove", "Gillette", "Bausch&Lomb"]
                for candidate in brand_candidates:
                    if candidate.lower() in name.lower():
                        brand = candidate
                        break
            
            # EAN
            ean_match = re.search(r'EAN:?\s*(\d{13})', soup.get_text())
            ean = ean_match.group(1) if ean_match else None
            
            # Promotion badge
            promo_element = soup.select_one(".product-badge__description")
            promotion = None
            if promo_element:
                promo_text = promo_element.get_text(strip=True)
                promotion = promo_text if promo_text else None
            
            # Stock status
            stock_level = 0
            stock_status = "Unknown"
            out_of_stock_element = soup.select_one(".out-of-stock-message, .c-info-message--out-of-stock")
            if out_of_stock_element:
                stock_status = "outOfStock"
            else:
                add_to_cart_element = soup.select_one("button.quantity-selector__add-to-cart-button:not([disabled])")
                if add_to_cart_element:
                    stock_status = "inStock"
                    stock_level = 1
            
            # Image URLs
            image_elements = soup.select(".product-image-carousel__image")
            image_urls = []
            for img in image_elements:
                src = img.get("src")
                if src:
                    # Ensure full URL
                    if src.startswith('/'):
                        src = f"https://www.etos.nl{src}"
                    image_urls.append(src)
            
            # Category breadcrumbs
            breadcrumb_elements = soup.select(".c-breadcrumb a")
            categories = []
            for elem in breadcrumb_elements:
                category_text = elem.get_text(strip=True)
                if category_text:
                    categories.append(category_text)
            
            # Price per unit
            price_per_unit_element = soup.select_one(".price__per-unit")
            price_per_unit = None
            if price_per_unit_element:
                price_per_unit_text = price_per_unit_element.get_text(strip=True)
                price_per_unit = price_per_unit_text if price_per_unit_text else None
            
            # Build final product data
            product_data = {
                "id": product_id,
                "url": url,
                "name": name or "Unknown Product",
                "brand": brand,
                "price": price,
                "original_price": original_price,
                "price_per_unit": price_per_unit,
                "content": content,
                "description": description,
                "ean": ean,
                "promotion": promotion,
                "stock_level": stock_level,
                "stock_status": stock_status,
                "image_urls": image_urls,
                "categories": categories,
                "store_id": "etos",
                "scraped_at": datetime.now().isoformat()
            }
            
            # Only return if we have at least a name and either price or description
            if product_data.get("name") and (product_data.get("price") or product_data.get("description")):
                return product_data
            else:
                logger.warning(f"Insufficient product data extracted from {url}")
                return None
            
        except Exception as e:
            logger.error(f"Error extracting data with CSS for product ID {product_id}: {str(e)}")
            return None
    
    def _parse_price(self, price_text: str) -> Optional[float]:
        """Parse price text to a float value"""
        if not price_text:
            return None
        
        # Clean and normalize the price text
        price_text = price_text.strip().replace(',', '.').replace('‚Ç¨', '')
        
        # Extract digits and decimal point
        price_match = re.search(r'(\d+)[.,]?(\d*)', price_text)
        if price_match:
            price_whole = price_match.group(1)
            price_decimal = price_match.group(2) or '0'
            
            # Handle case when there's no decimal part or it's incomplete
            if len(price_decimal) == 1:
                price_decimal += '0'
            
            try:
                return float(f"{price_whole}.{price_decimal}")
            except ValueError:
                return None
        
        return None

    async def _initialize_scraping(self, state: EtosScrapingState) -> EtosScrapingState:
        """Initialize the scraping session"""
        logger.info(f"üöÄ Starting {state.store_name} scraping session")
        
        state.status = AgentStatus.INITIALIZING
        state.start_time = datetime.now()
        state.products = []
        state.errors = []
        
        # Create session
        await self.create_session()
        
        logger.info("‚úÖ Etos agent initialized")
        return state
    
    async def _fetch_product_urls(self, state: EtosScrapingState) -> EtosScrapingState:
        """Fetch product URLs from Etos sitemap"""
        logger.info("üìÇ Fetching Etos product URLs from sitemap")
        
        try:
            # Extract product URLs from XML sitemap
            product_urls = await self.extract_product_urls_from_xml(state.sitemap_url)
            
            if product_urls:
                # For testing, limit to first 20 products
                state.product_urls = product_urls[:20]
                state.current_batch_num = 0
                
                logger.info(f"‚úÖ Found {len(state.product_urls)} product URLs (limited for testing)")
            else:
                logger.error("‚ùå No product URLs found in sitemap")
                state.product_urls = []
                
        except Exception as e:
            state.errors.append({
                "step": "fetch_product_urls",
                "error": str(e),
                "timestamp": datetime.now()
            })
            logger.error(f"‚ùå Error fetching product URLs: {e}")
            state.product_urls = []
        
        return state
    
    async def _process_batch(self, state: EtosScrapingState) -> EtosScrapingState:
        """Process a batch of product URLs"""
        if not state.product_urls:
            return state
            
        # Calculate batch boundaries
        start_idx = state.current_batch_num * self.batch_size
        end_idx = min(start_idx + self.batch_size, len(state.product_urls))
        
        if start_idx >= len(state.product_urls):
            logger.info("üéâ All batches processed")
            return state
        
        batch_urls = state.product_urls[start_idx:end_idx]
        state.current_url_batch = batch_urls
        
        logger.info(f"üõçÔ∏è Processing batch {state.current_batch_num + 1}, URLs {start_idx+1}-{end_idx}")
        
        try:
            batch_products = []
            
            # Process each URL in the batch
            for url in batch_urls:
                try:
                    product_data = await self.extract_product_data(url)
                    if product_data:
                        batch_products.append(product_data)
                        
                except Exception as e:
                    logger.error(f"‚ùå Error processing URL {url}: {e}")
                    state.errors.append({
                        "step": "process_product",
                        "url": url,
                        "error": str(e),
                        "timestamp": datetime.now()
                    })
                    continue
            
            # Add successful products to state
            state.products.extend(batch_products)
            state.current_batch_num += 1
            state.batches_processed += 1
            state.products_scraped = len(state.products)
            
            logger.info(f"‚úÖ Batch completed: {len(batch_products)} products extracted")
            logger.info(f"üìà Total products so far: {state.products_scraped}")
            
        except Exception as e:
            state.errors.append({
                "step": "process_batch",
                "batch_num": state.current_batch_num,
                "error": str(e),
                "timestamp": datetime.now()
            })
            logger.error(f"‚ùå Error processing batch: {e}")
            state.retry_count += 1
        
        return state

    async def _handle_rate_limit(self, state: EtosScrapingState) -> EtosScrapingState:
        """Handle rate limiting"""
        wait_time = min(state.rate_limit_delay * (2 ** state.retry_count), 60)
        logger.info(f"‚è±Ô∏è Rate limit hit, waiting {wait_time}s")
        await asyncio.sleep(wait_time)
        return state

    async def _handle_error(self, state: EtosScrapingState) -> EtosScrapingState:
        """Handle scraping errors"""
        if state.retry_count < state.max_retries:
            logger.info(f"üîÑ Retrying after error (attempt {state.retry_count + 1}/{state.max_retries})")
            await asyncio.sleep(2 * state.retry_count)
        else:
            logger.info(f"‚ùå Max retries reached, moving to next batch")
            state.current_batch_num += 1  # Skip this batch
            state.retry_count = 0
        return state

    async def _complete_scraping(self, state: EtosScrapingState) -> EtosScrapingState:
        """Complete the scraping session"""
        end_time = datetime.now()
        duration = end_time - state.start_time
        
        logger.info(f"üéâ Etos scraping completed!")
        logger.info(f"üìä Total products scraped: {len(state.products)}")
        logger.info(f"‚è±Ô∏è Duration: {duration}")
        logger.info(f"‚ùå Errors: {len(state.errors)}")
        logger.info(f"‚úÖ Successful requests: {self.successful_requests}")
        logger.info(f"‚ùå Failed requests: {self.failed_requests}")
        logger.info(f"‚è≥ Rate limited: {self.rate_limited}")
        
        # Close session
        await self.close_session()
        
        state.status = AgentStatus.COMPLETED
        state.end_time = end_time
        state.duration = duration.total_seconds()
        
        return state

    def _should_process_products(self, state: EtosScrapingState) -> str:
        """Check if we should process products"""
        if not state.product_urls:
            return "complete"
        return "process"

    def _check_batch_result(self, state: EtosScrapingState) -> str:
        """Check the result of batch processing"""
        # Check if we've processed all batches
        if state.current_batch_num * self.batch_size >= len(state.product_urls):
            return "complete"
        
        # Check for rate limiting
        if self.rate_limited > 0 and state.retry_count < state.max_retries:
            return "rate_limit"
        
        # Check for errors
        if state.retry_count >= state.max_retries:
            return "error"
        
        # Continue processing
        return "continue"

    async def run_scraping_job(self, job_config: Dict[str, Any]) -> Dict[str, Any]:
        """Run a complete scraping job for Etos"""
        try:
            logger.info("üöÄ Starting real Etos scraping job")
            
            # Initialize state
            self.state.start_time = datetime.now()
            self.state.status = AgentStatus.RUNNING
            self.state.store_name = "Etos"
            self.state.store_slug = "etos"
            
            # Reset progress tracking
            self.successful_requests = 0
            self.failed_requests = 0
            self.rate_limited = 0
            
            # Create session
            await self.create_session()
            
            # For testing, run a limited scraping job
            try:
                # Fetch product URLs from sitemap
                product_urls = await self.extract_product_urls_from_xml(self.state.sitemap_url)
                
                if not product_urls:
                    return {
                        "success": False,
                        "message": "No product URLs found in sitemap",
                        "products_scraped": 0,
                        "langgraph_used": True,
                        "real_scraping": True
                    }
                
                # Process first 5 products for testing
                test_urls = product_urls[:5]
                scraped_products = []
                
                for url in test_urls:
                    product_data = await self.extract_product_data(url)
                    if product_data:
                        scraped_products.append(product_data)
                
                # Update state
                self.state.status = AgentStatus.COMPLETED
                self.state.end_time = datetime.now()
                self.state.products_scraped = len(scraped_products)
                
                return {
                    "success": True,
                    "message": f"Real Etos scraping completed",
                    "products_scraped": len(scraped_products),
                    "total_urls_found": len(product_urls),
                    "successful_requests": self.successful_requests,
                    "failed_requests": self.failed_requests,
                    "rate_limited": self.rate_limited,
                    "products_sample": scraped_products[:2],  # Return first 2 as sample
                    "langgraph_used": True,
                    "real_scraping": True
                }
                
            except Exception as e:
                logger.error(f"‚ùå Error in scraping job: {e}")
                return {
                    "success": False,
                    "message": f"Scraping job failed: {str(e)}",
                    "langgraph_used": True,
                    "real_scraping": True
                }
            
            finally:
                await self.close_session()
            
        except Exception as e:
            return {
                "success": False,
                "message": f"Job initialization failed: {str(e)}",
                "langgraph_used": True,
                "real_scraping": True
            } 